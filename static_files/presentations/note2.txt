1
01:28:20,000 --> 02:22:13,333
同学们欢迎回到自然园处理这门课

2
02:35:00,000 --> 02:53:20,000
啊这节课呢

3
02:53:20,000 --> 03:32:46,666
我给大家介绍一下我们课程的

4
03:32:46,666 --> 03:50:33,333
总体框架

5
04:04:26,666 --> 05:17:13,333
下面呢是一个课程大概的组织架构

6
05:45:00,000 --> 06:34:26,666
我们这个课程分为三大部分

7
06:56:40,000 --> 07:15:33,333
在第一部分

8
07:20:33,333 --> 08:02:13,333
我们介绍一些基本的概念

9
08:53:20,000 --> 09:07:46,666
首先呢

10
09:26:06,666 --> 09:49:26,666
在这节课里面

11
10:11:40,000 --> 11:00:33,333
我们有一个课程的整体的框架

12
11:35:00,000 --> 12:17:46,666
那么在后面的几节课里

13
12:32:46,666 --> 13:18:53,333
我将给自然语言处理是什么

14
13:27:13,333 --> 14:15:00,000
还有自然语言处理所研究的主要任务

15
14:22:46,666 --> 15:16:40,000
以及为什么要以机器学习的角度

16
15:19:26,666 --> 15:50:33,333
来享受自然语言处理

17
16:03:53,333 --> 16:31:40,000
给一个深入的介绍

18
17:23:20,000 --> 17:49:26,666
之后的几节课呢

19
17:56:40,000 --> 18:19:26,666
我们将学习到

20
18:33:53,333 --> 19:17:13,333
数数的基本统计方法

21
19:23:53,333 --> 19:52:13,333
和数学建模的思想

22
20:13:53,333 --> 20:34:26,666
我们管这几节课

23
20:34:26,666 --> 21:19:26,666
叫 counting relative frequencies

24
21:48:20,000 --> 22:15:33,333
这里的模型基本上

25
22:21:40,000 --> 23:03:20,000
都是通过统计意义上的频率

26
23:11:40,000 --> 24:02:46,666
相对数量来对概率进行建模的

27
24:44:26,666 --> 25:10:00,000
在下面几节课呢

28
25:21:06,666 --> 25:56:40,000
我们将介绍一种

29
25:59:26,666 --> 26:30:33,333
叫做判别式模型的

30
26:32:46,666 --> 27:02:13,333
自然语言处理建模方法

31
27:31:40,000 --> 27:44:26,666
其中呢

32
27:47:46,666 --> 28:43:20,000
最重要的统计知识叫做特征项链

33
29:16:06,666 --> 29:47:46,666
我们会以项链空间

34
29:47:46,666 --> 30:15:00,000
为最基本的数学模型

35
30:21:06,666 --> 31:03:20,000
对自然语言处理的任务进行建模

36
31:20:33,333 --> 31:57:46,666
那我们将讨论以这个切入点

37
32:06:40,000 --> 33:00:00,000
为核心的所有相关的机器学习方法

38
33:58:20,000 --> 34:24:26,666
在下面几节课呢

39
34:43:53,333 --> 35:27:13,333
我们叫 unified linear models

40
36:05:33,333 --> 36:20:00,000
我们会把

41
36:42:13,333 --> 37:17:46,666
上一节课介绍的这些

42
37:33:53,333 --> 38:04:26,666
基于特征项链的模型

43
38:25:33,333 --> 38:56:06,666
和概率统计的模型

44
39:10:00,000 --> 39:58:20,000
统一到完全相同的一个

45
40:06:06,666 --> 40:25:00,000
框架底下

46
40:35:00,000 --> 41:05:00,000
这个框架呢我管他叫

47
41:23:20,000 --> 42:01:40,000
泛化的感知机模型

48
42:16:40,000 --> 42:51:06,666
也就是单层的神经网络

49
43:06:40,000 --> 43:41:40,000
我们会讨论这个最基本的

50
43:46:06,666 --> 44:18:20,000
线性模型的优化方式

51
44:38:53,333 --> 45:21:40,000
以及相关的机器学习的一些

52
45:26:40,000 --> 45:50:00,000
最基本的概念

53
45:53:20,000 --> 46:40:33,333
比如过拟核和泛化性问题

54
47:30:33,333 --> 47:57:13,333
在下面几节课呢

55
48:18:20,000 --> 48:31:40,000
我们会

56
48:45:33,333 --> 49:17:46,666
把上几节课的

57
49:25:33,333 --> 50:03:53,333
一个泛画的感知金模型

58
50:11:06,666 --> 50:41:40,000
进行一个理论上的升华

59
51:13:20,000 --> 51:45:00,000
我们会对数数的模型

60
51:50:00,000 --> 52:21:40,000
也进行一个理论的升华

61
52:43:53,333 --> 53:27:46,666
最后发现所有这些模型背后的

62
53:33:20,000 --> 54:12:13,333
嗯一些数学意义呢

63
54:23:53,333 --> 55:04:26,666
他可以归结于信息论的范畴

64
55:29:26,666 --> 55:51:06,666
在这节课里呢

65
55:55:00,000 --> 56:16:06,666
我们将会研究

66
56:16:06,666 --> 56:56:06,666
怎么用信息论里面的基本概念

67
56:59:26,666 --> 57:32:46,666
比如信息商

68
57:41:40,000 --> 58:25:00,000
困惑度等等这些概念

69
58:30:33,333 --> 59:39:26,666
我们推出前面几节课的各大类模型

70
60:09:26,666 --> 60:32:46,666
并且我们还介绍

71
60:40:33,333 --> 61:16:06,666
这些基本的信息论概念

72
61:19:26,666 --> 62:15:00,000
在自然语言处理里面的大体应用方向

73
63:15:33,333 --> 63:50:00,000
第一部分的最后几节课呢

74
63:56:40,000 --> 64:15:00,000
我们将引入

75
64:25:33,333 --> 64:52:46,666
引变量这个概念

76
65:41:06,666 --> 66:20:33,333
就是机器学习的训练数据里

77
66:27:46,666 --> 67:01:06,666
并不能直接观察到的

78
67:07:13,333 --> 67:41:40,000
这样一些数据和变量

79
68:17:13,333 --> 68:36:40,000
我们会介绍

80
68:43:53,333 --> 69:35:00,000
解决引边量问题的最基本的思想

81
70:04:26,666 --> 71:03:20,000
并且呢引入一个叫做期望最大

82
71:18:20,000 --> 71:52:13,333
expectation maximization

83
71:57:13,333 --> 72:26:40,000
也就是 em 的算法

84
72:48:53,333 --> 73:02:13,333
我们会看到

85
73:02:13,333 --> 73:51:06,666
这个算法在自然语言处理领域到

86
74:01:40,000 --> 74:25:00,000
一些典型应用

87
74:47:13,333 --> 75:01:06,666
并且呢

88
75:08:53,333 --> 76:06:40,000
我们会根据前面这些课学到的基本

89
76:10:33,333 --> 76:44:26,666
概率和信息论的知识

90
76:48:53,333 --> 77:37:13,333
来推导出 em 算法本身

91
78:32:13,333 --> 79:04:26,666
那第一部分基本上是

92
79:05:00,000 --> 79:36:06,666
自然语言处理模型的

93
79:43:20,000 --> 79:54:26,666
基础

94
80:07:46,666 --> 80:27:46,666
这些基础呢

95
80:33:20,000 --> 81:17:13,333
决定了我们对第二部分

96
81:18:20,000 --> 81:42:46,666
第三部分的学习

97
82:27:13,333 --> 82:48:20,000
在第二部分呢

98
82:55:33,333 --> 83:13:53,333
我们将会介绍

99
83:13:53,333 --> 83:57:13,333
自然语言处理非常独特的一些

100
84:00:33,333 --> 84:22:13,333
机器学习算法

101
84:38:20,000 --> 85:03:53,333
为什么说独特呢

102
85:07:13,333 --> 85:39:26,666
因为这些机器学习算法

103
85:53:53,333 --> 86:22:46,666
大概上有别于

104
86:41:06,666 --> 86:59:26,666
图像处理

105
87:15:33,333 --> 88:05:33,333
等其他机器学习的应用领域

106
88:47:46,666 --> 89:35:33,333
我们发现自然语言里面很多现象

107
90:01:06,666 --> 90:22:13,333
是结构现象

108
90:41:06,666 --> 90:57:13,333
比如一句话

109
90:58:53,333 --> 91:26:40,000
他就是一个词的序列

110
91:38:53,333 --> 92:03:53,333
他本身是一个结构

111
92:30:33,333 --> 93:05:00,000
你要深入理解这句话呢

112
93:10:33,333 --> 93:41:06,666
你需要一些句法上的

113
93:43:53,333 --> 94:14:26,666
语意上的结构表达

114
94:18:53,333 --> 94:47:46,666
这些语言学的结构呢

115
94:56:06,666 --> 95:18:53,333
也属于结构问题

116
95:56:40,000 --> 96:24:26,666
如何让机器学习算法

117
96:24:26,666 --> 96:50:00,000
处理好这些结构

118
96:52:46,666 --> 97:15:33,333
是我们第二部分

119
97:22:46,666 --> 97:47:46,666
所讲述的内容

120
98:27:13,333 --> 98:47:46,666
前几节课呢

121
99:03:53,333 --> 99:29:26,666
我们首先把

122
99:51:40,000 --> 100:25:00,000
带绿的数数的模型

123
100:47:13,333 --> 101:07:46,666
从简单的

124
101:20:33,333 --> 101:40:33,333
分类的问题

125
102:20:33,333 --> 102:35:33,333
推演成

126
102:47:46,666 --> 103:38:20,000
如何解决序列结构的问题

127
104:16:40,000 --> 104:58:20,000
我们管这些模型叫生成模型

128
105:28:53,333 --> 105:41:06,666
所以呢

129
105:42:46,666 --> 106:47:13,333
这些课程讨论的是生成序列标注问题

130
107:12:46,666 --> 107:43:20,000
有一个最典型的模型

131
107:46:40,000 --> 108:21:40,000
叫做饮马尔可夫模型

132
109:07:13,333 --> 109:31:40,000
下面几节课呢

133
109:40:00,000 --> 109:59:26,666
我们会讨论

134
110:21:40,000 --> 110:48:53,333
限量空间模型

135
111:10:00,000 --> 111:48:53,333
对于同样的问题的处理方式

136
112:07:46,666 --> 112:22:46,666
也就是说

137
112:31:06,666 --> 112:46:06,666
我们会把

138
113:07:46,666 --> 113:27:13,333
判别模型

139
113:54:26,666 --> 114:32:46,666
用到序列标注上

140
114:41:40,000 --> 115:26:40,000
那么这里面有一个典型的模型呢

141
115:37:13,333 --> 116:06:06,666
叫做条件随机场

142
116:17:13,333 --> 116:45:00,000
肯定是呢 random field

143
117:04:26,666 --> 117:26:40,000
或者 crf

144
118:28:53,333 --> 118:54:26,666
在下面几节课呢

145
119:11:40,000 --> 119:48:53,333
我们会讨论一种新的结构

146
120:02:46,666 --> 120:27:13,333
叫做序列切分

147
120:53:53,333 --> 121:28:53,333
其中有一个典型的例子呢

148
121:43:53,333 --> 122:01:06,666
就是中文

149
122:19:26,666 --> 122:32:13,333
一句话

150
122:52:46,666 --> 123:36:40,000
他由一个字符的序列构成

151
124:05:00,000 --> 124:37:46,666
如何把这个字符的序列

152
124:48:53,333 --> 125:16:06,666
切成词的序列

153
125:30:33,333 --> 125:49:26,666
这么样的问题

154
126:31:40,000 --> 127:10:33,333
我们会针对这样的问题呢

155
127:31:40,000 --> 128:20:33,333
把过去学过的概率的生成模型

156
128:25:33,333 --> 129:06:06,666
或者集于项链的判别模型

157
129:15:00,000 --> 129:41:06,666
都分别对他建模

158
130:12:13,333 --> 130:47:13,333
这里面一个典型的模型呢

159
130:55:00,000 --> 131:32:13,333
叫做半条件随机场

160
131:38:20,000 --> 132:12:13,333
或者3米 mark of crf

161
133:07:46,666 --> 133:33:20,000
在下面几节课呢

162
133:36:40,000 --> 134:15:00,000
我们继续结构的演化

163
134:23:20,000 --> 135:00:33,333
我们会讨论竖状结构

164
135:10:33,333 --> 135:47:46,666
是如何去进行生成的

165
136:17:13,333 --> 136:43:20,000
我们会把同样的

166
136:46:40,000 --> 137:31:06,666
生成式模型和判别式模型

167
137:59:26,666 --> 139:00:00,000
改造成对于竖状结构的预测模型

168
139:40:33,333 --> 140:17:46,666
这里面一个典型的结构呢

169
140:23:20,000 --> 140:47:46,666
是锯法的结构

170
141:51:40,000 --> 142:21:40,000
一个典型的模型呢

171
142:41:06,666 --> 143:31:40,000
是概率的上下文无关语法模型

172
144:12:46,666 --> 144:48:20,000
另外呢条件随机场

173
144:52:46,666 --> 145:43:53,333
也会被泛化成树状的条件随机场

174
146:52:46,666 --> 147:17:13,333
在下面几节课呢

175
147:39:26,666 --> 148:02:46,666
我们会研究一下

176
148:33:20,000 --> 148:55:33,333
结构预测里面

177
149:08:53,333 --> 149:28:53,333
算法复杂度

178
149:42:46,666 --> 149:50:00,000
和

179
150:04:26,666 --> 150:22:13,333
模型精度

180
150:29:26,666 --> 151:27:13,333
或者说你的特征销量空间之间的关系

181
151:45:33,333 --> 152:28:53,333
并且呢我们将讨论一类

182
152:52:13,333 --> 153:02:13,333
能够

183
153:31:40,000 --> 154:16:40,000
中和算法复杂度和

184
154:36:40,000 --> 155:35:33,333
限量空间复杂度的内在矛盾的算法

185
155:58:20,000 --> 156:50:00,000
也就是基于转移的结构预测算法

186
158:10:33,333 --> 158:36:40,000
在下面几节课呢

187
158:58:20,000 --> 159:18:53,333
我们将讨论

188
159:35:00,000 --> 160:22:13,333
生成式的结构预测模型的一个

189
160:46:06,666 --> 161:25:00,000
很泛化的普遍的版本

190
161:34:26,666 --> 162:13:53,333
叫做贝耶斯学习

191
162:22:13,333 --> 162:53:20,000
或者叫做贝耶斯网络

192
163:20:33,333 --> 163:45:00,000
我们会了解到

193
164:02:13,333 --> 164:55:00,000
当训练数据里面有隐边亮的时候

194
165:07:13,333 --> 165:45:00,000
被夜思网络如何去学习

195
166:12:46,666 --> 166:57:46,666
我们也会对比贝叶斯学习

196
167:07:13,333 --> 168:11:40,000
和前面基于数数的统计学习之间的

197
168:28:20,000 --> 168:57:46,666
相同点和不同点

198
170:24:26,666 --> 170:53:53,333
在课程的最后一部分

199
170:53:53,333 --> 171:18:20,000
也就是第三部分

200
171:44:26,666 --> 172:03:20,000
我们会介绍

201
172:20:00,000 --> 173:26:40,000
神经网络对于自然处理建模的作用

202
173:56:06,666 --> 174:43:53,333
这一部分将以第一部分所介绍的

203
174:46:40,000 --> 175:06:06,666
概率知识

204
175:12:46,666 --> 175:35:00,000
信息论知识

205
175:42:46,666 --> 176:24:26,666
引边量知识为基础

206
176:46:40,000 --> 177:29:26,666
把第一部分介绍的

207
177:32:46,666 --> 178:30:00,000
单层的感知机模型以及他的优化算法

208
178:40:33,333 --> 179:34:26,666
扩展到多层感知机模型

209
179:39:26,666 --> 180:16:06,666
也就是 multi layer perceptron

210
180:25:00,000 --> 181:16:06,666
以及更复杂的网络结构里

211
181:33:53,333 --> 181:52:13,333
我们会发现

212
182:01:06,666 --> 182:36:40,000
所有这些模型内在上

213
182:48:53,333 --> 183:06:40,000
是统一的

214
183:19:26,666 --> 183:36:06,666
和整体的

215
184:10:33,333 --> 184:56:40,000
在这一部分的前几节课里呢

216
185:13:20,000 --> 185:40:00,000
我们会给大家讨论

217
185:50:00,000 --> 186:09:26,666
如何把一个

218
186:18:53,333 --> 186:39:26,666
线性的模型

219
186:50:33,333 --> 187:00:33,333
通过

220
187:17:13,333 --> 187:55:33,333
层数堆叠的方法变成一个

221
187:58:53,333 --> 188:21:40,000
非线性的模型

222
188:59:26,666 --> 189:23:53,333
我们会介绍一个

223
189:35:00,000 --> 190:15:33,333
最基本的最简单的

224
190:37:46,666 --> 190:57:13,333
用神经网络

225
191:11:06,666 --> 192:02:46,666
去做文本分类的解决分类任务的模型

226
192:53:20,000 --> 193:22:13,333
在下面几节课里呢

227
193:43:53,333 --> 194:10:00,000
我们将通过对比

228
194:23:53,333 --> 195:02:13,333
单层模型和多层模型

229
195:20:33,333 --> 195:36:06,666
背后的

230
195:48:20,000 --> 196:07:13,333
能力差别

231
196:33:20,000 --> 196:58:53,333
着重的讨论

232
197:07:13,333 --> 197:25:33,333
神经网络

233
197:40:33,333 --> 198:05:00,000
的一个关键技术

234
198:19:26,666 --> 198:43:53,333
叫做表示学习

235
199:18:20,000 --> 199:38:53,333
我们会了解

236
199:58:20,000 --> 200:26:06,666
表示学习的含义

237
200:46:40,000 --> 201:08:20,000
以及不同的

238
201:16:06,666 --> 201:28:20,000
实现

239
201:40:33,333 --> 202:05:33,333
自然语言处理

240
202:12:13,333 --> 202:56:06,666
表示学习的神经网络架构

241
203:34:26,666 --> 203:59:26,666
以及这些架构的

242
204:12:13,333 --> 204:36:40,000
不同优化方法

243
205:33:20,000 --> 206:02:46,666
在下面的几节课呢

244
206:26:06,666 --> 207:23:53,333
我们会针对第二部分所讨论的

245
207:36:06,666 --> 208:22:13,333
不同的特点的结构

246
208:48:20,000 --> 209:22:13,333
进行神经网络的建模

247
209:58:20,000 --> 210:11:40,000
我们会

248
210:25:00,000 --> 210:56:40,000
把基于转移的方法

249
211:12:46,666 --> 211:53:53,333
和非基于转移的方法

250
212:02:46,666 --> 212:26:40,000
都神经网络换

251
212:48:20,000 --> 213:14:26,666
并通过对比

252
213:45:33,333 --> 214:22:13,333
来观察到神经网络解决局

253
214:22:13,333 --> 214:48:53,333
结构预测问题的

254
215:04:26,666 --> 215:15:33,333
特点

255
216:28:20,000 --> 216:57:13,333
那么在下面几节课呢

256
217:19:26,666 --> 217:48:53,333
我们会介绍神经网络

257
217:56:40,000 --> 218:29:26,666
由于他的强大表达能力

258
218:40:00,000 --> 219:21:06,666
所能做到的传统统计模型

259
219:21:06,666 --> 219:50:33,333
难以做到的一些任务

260
220:01:06,666 --> 220:13:20,000
其中呢

261
220:33:20,000 --> 221:01:40,000
很经典的任务就是

262
221:12:46,666 --> 221:44:26,666
对两个不同的文本

263
221:53:20,000 --> 222:25:00,000
进行复杂建模的任务

264
222:47:13,333 --> 223:26:40,000
比如我们将研究序列到序列

265
223:43:20,000 --> 224:32:13,333
sequence to sequence 这样的模型

266
224:46:40,000 --> 225:46:06,666
他可以把一个文本转换成另一个文本

267
226:05:33,333 --> 226:32:46,666
我们也将介绍

268
226:44:26,666 --> 227:38:20,000
对两个文本之间进行对比

269
228:16:06,666 --> 228:55:33,333
和理解的这样的模型

270
228:55:33,333 --> 229:28:53,333
比如说啊一个

271
229:43:53,333 --> 230:22:13,333
根据文本的问答模型

272
231:15:00,000 --> 231:41:06,666
啊下面几节课呢

273
232:00:00,000 --> 232:44:26,666
我们还是针对神经网络的特点

274
233:01:40,000 --> 233:15:33,333
来介绍

275
233:31:06,666 --> 233:58:53,333
玉训练这个概念

276
234:20:00,000 --> 235:21:06,666
以及用神经网络如何进行迁移学习

277
235:26:06,666 --> 235:40:00,000
也就是说

278
235:42:46,666 --> 236:13:53,333
把一个问题学到的知识

279
236:15:33,333 --> 236:44:26,666
用到另一个问题上的

280
236:46:06,666 --> 237:25:00,000
所有基本的这些想法和思路

281
238:33:20,000 --> 239:29:26,666
在我们课程的最后几个部分呢

282
239:51:06,666 --> 240:17:46,666
我们会重点讨论

283
240:25:00,000 --> 241:05:33,333
如何用神经网络

284
241:26:06,666 --> 241:41:40,000
来解决

285
241:53:53,333 --> 242:27:46,666
隐变量的建模问题

286
242:58:53,333 --> 243:30:33,333
在这一部分内容里面呢

287
243:41:06,666 --> 244:15:33,333
我们既会讨论分类问题

288
244:15:33,333 --> 244:48:20,000
也会讨论结构预测问题

289
245:08:53,333 --> 245:22:13,333
但是呢

290
245:30:33,333 --> 246:33:20,000
我们的讨论着重点在于一个泛化的

291
247:02:13,333 --> 247:38:20,000
期望最大就是 em

292
247:48:20,000 --> 248:02:46,666
方法上

293
248:48:20,000 --> 249:13:20,000
我们不会去讨论

294
249:26:40,000 --> 250:03:53,333
神经网络的贝叶斯方法

295
250:12:46,666 --> 251:05:00,000
因为这些方法目前在自然语言处理中

296
251:07:46,666 --> 251:36:40,000
应用还相对比较少

297
253:25:00,000 --> 253:54:26,666
这部分的内容就讲完了

298
253:56:06,666 --> 254:14:26,666
咱们下次再见

